{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e8bb76d4",
   "metadata": {},
   "source": [
    "D:\\\\Human Emotion Recognition\\\\RAF DB\\\\RAF Models\\\\best_model_efficientnet_raf_after_aug.h5\n",
    "D:\\\\Human Emotion Recognition\\\\RAF DB\\\\RAF Models\\\\mobilenet_raf_fold2.h5\n",
    "D:\\\\Human Emotion Recognition\\\\Emotion Recognition Datasets\\\\Affectnet Models\\\\patch_model_raf.h5\n",
    "D:\\\\Human Emotion Recognition\\\\RAF DB\\\\Data Preparation\\\\best_model_efficientnet_before_aug.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e2b08",
   "metadata": {},
   "source": [
    "# Real Time Emotion Recognition From Facial Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e826c256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\226735 C2IN\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "neutral (99.74%)\n",
      "1/1 [==============================] - 2.019s 0ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "neutral (98.19%)\n",
      "1/1 [==============================] - 0.096s 10ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "neutral (97.90%)\n",
      "1/1 [==============================] - 0.122s 8ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "neutral (98.62%)\n",
      "1/1 [==============================] - 0.094s 10ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "neutral (99.28%)\n",
      "1/1 [==============================] - 0.089s 11ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "neutral (99.28%)\n",
      "1/1 [==============================] - 0.108s 9ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "neutral (97.88%)\n",
      "1/1 [==============================] - 0.110s 9ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "neutral (74.28%)\n",
      "1/1 [==============================] - 0.108s 9ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "neutral (96.03%)\n",
      "1/1 [==============================] - 0.110s 9ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "neutral (95.14%)\n",
      "1/1 [==============================] - 0.107s 9ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "neutral (91.09%)\n",
      "1/1 [==============================] - 0.116s 8ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "neutral (94.39%)\n",
      "1/1 [==============================] - 0.113s 8ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "neutral (92.39%)\n",
      "1/1 [==============================] - 0.112s 8ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "neutral (75.79%)\n",
      "1/1 [==============================] - 0.107s 9ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "neutral (82.34%)\n",
      "1/1 [==============================] - 0.109s 9ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "neutral (87.91%)\n",
      "1/1 [==============================] - 0.105s 9ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "neutral (80.18%)\n",
      "1/1 [==============================] - 0.104s 9ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "neutral (85.54%)\n",
      "1/1 [==============================] - 0.114s 8ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "neutral (90.11%)\n",
      "1/1 [==============================] - 0.129s 7ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "neutral (97.74%)\n",
      "1/1 [==============================] - 0.146s 6ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "neutral (99.44%)\n",
      "1/1 [==============================] - 0.129s 7ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "neutral (99.03%)\n",
      "1/1 [==============================] - 0.114s 8ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "neutral (99.67%)\n",
      "1/1 [==============================] - 0.125s 7ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "neutral (99.38%)\n",
      "1/1 [==============================] - 0.129s 7ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "neutral (98.86%)\n",
      "1/1 [==============================] - 0.159s 6ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "neutral (98.40%)\n",
      "1/1 [==============================] - 0.183s 5ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "neutral (98.59%)\n",
      "1/1 [==============================] - 0.172s 5ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "neutral (97.31%)\n",
      "1/1 [==============================] - 0.087s 11ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "neutral (97.23%)\n",
      "1/1 [==============================] - 0.084s 11ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "neutral (97.85%)\n",
      "1/1 [==============================] - 0.114s 8ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "neutral (99.40%)\n",
      "1/1 [==============================] - 0.111s 9ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "neutral (96.86%)\n",
      "1/1 [==============================] - 0.113s 8ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "happy (95.82%)\n",
      "1/1 [==============================] - 0.107s 9ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "neutral (79.27%)\n",
      "1/1 [==============================] - 0.109s 9ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "neutral (80.68%)\n",
      "1/1 [==============================] - 0.108s 9ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "happy (68.68%)\n",
      "1/1 [==============================] - 0.113s 8ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "happy (100.00%)\n",
      "1/1 [==============================] - 0.113s 8ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "happy (99.89%)\n",
      "1/1 [==============================] - 0.107s 9ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "happy (99.93%)\n",
      "1/1 [==============================] - 0.109s 9ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "happy (99.99%)\n",
      "1/1 [==============================] - 0.107s 9ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "angry (68.07%)\n",
      "1/1 [==============================] - 0.098s 10ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "neutral (76.47%)\n",
      "1/1 [==============================] - 0.102s 9ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "neutral (96.64%)\n",
      "1/1 [==============================] - 0.115s 8ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "neutral (87.99%)\n",
      "1/1 [==============================] - 0.111s 8ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "happy (100.00%)\n",
      "1/1 [==============================] - 0.110s 9ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "happy (99.97%)\n",
      "1/1 [==============================] - 0.126s 7ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "neutral (60.00%)\n",
      "1/1 [==============================] - 0.108s 9ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "happy (100.00%)\n",
      "1/1 [==============================] - 0.111s 9ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "neutral (98.76%)\n",
      "1/1 [==============================] - 0.106s 9ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "neutral (96.92%)\n",
      "1/1 [==============================] - 0.110s 9ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "neutral (99.17%)\n",
      "1/1 [==============================] - 0.111s 9ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "neutral (99.26%)\n",
      "1/1 [==============================] - 0.098s 10ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "neutral (99.72%)\n",
      "1/1 [==============================] - 0.112s 8ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "neutral (99.75%)\n",
      "1/1 [==============================] - 0.123s 8ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "neutral (51.22%)\n",
      "1/1 [==============================] - 0.102s 9ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "happy (99.35%)\n",
      "1/1 [==============================] - 0.104s 9ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "happy (99.94%)\n",
      "1/1 [==============================] - 0.113s 8ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "happy (100.00%)\n",
      "1/1 [==============================] - 0.105s 9ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "neutral (87.52%)\n",
      "1/1 [==============================] - 0.106s 9ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "neutral (88.22%)\n",
      "1/1 [==============================] - 0.108s 9ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "happy (100.00%)\n",
      "1/1 [==============================] - 0.104s 9ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "neutral (98.51%)\n",
      "1/1 [==============================] - 0.104s 9ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "happy (73.83%)\n",
      "1/1 [==============================] - 0.116s 8ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "neutral (87.07%)\n",
      "1/1 [==============================] - 0.108s 9ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "happy (99.55%)\n",
      "1/1 [==============================] - 0.106s 9ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "happy (100.00%)\n",
      "1/1 [==============================] - 0.106s 9ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "happy (79.31%)\n",
      "1/1 [==============================] - 0.109s 9ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "happy (100.00%)\n",
      "1/1 [==============================] - 0.105s 9ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "happy (99.93%)\n",
      "1/1 [==============================] - 0.104s 9ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "happy (100.00%)\n",
      "1/1 [==============================] - 0.110s 9ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "happy (69.13%)\n",
      "1/1 [==============================] - 0.106s 9ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "happy (92.36%)\n",
      "1/1 [==============================] - 0.107s 9ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "neutral (91.13%)\n",
      "1/1 [==============================] - 0.109s 9ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "happy (100.00%)\n",
      "1/1 [==============================] - 0.106s 9ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "neutral (68.28%)\n",
      "1/1 [==============================] - 0.107s 9ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "happy (98.89%)\n",
      "1/1 [==============================] - 0.104s 9ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "happy (99.93%)\n",
      "1/1 [==============================] - 0.111s 9ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "happy (100.00%)\n",
      "1/1 [==============================] - 0.110s 9ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "happy (96.09%)\n",
      "1/1 [==============================] - 0.108s 9ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "happy (95.36%)\n",
      "1/1 [==============================] - 0.105s 9ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "neutral (77.35%)\n",
      "1/1 [==============================] - 0.105s 9ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "neutral (57.89%)\n",
      "1/1 [==============================] - 0.115s 8ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "neutral (85.54%)\n",
      "1/1 [==============================] - 0.107s 9ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "neutral (98.29%)\n",
      "1/1 [==============================] - 0.108s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import face_detection\n",
    "import time\n",
    "\n",
    "# Load the pre-trained emotion classification model\n",
    "emotion_model = load_model('D:\\\\Human Emotion Recognition\\\\RAF DB\\\\RAF Models\\\\mobilenet_raf_fold2.h5')\n",
    "\n",
    "# Initialize the face detection model\n",
    "detector = face_detection.build_detector(\"RetinaNetResNet50\", confidence_threshold=.5, nms_iou_threshold=.3)\n",
    "\n",
    "# Specify the class names explicitly as strings in the order they were trained\n",
    "class_names = ['neutral', 'happy', 'sad', 'surprise', 'fear', 'disgust', 'angry']\n",
    "\n",
    "def classify_emotion(face_image):\n",
    "    # Convert color image to grayscale for the emotion recognition model\n",
    "    face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize the face image to the new target size of 100 x 100\n",
    "    face_image = cv2.resize(face_image, (100, 100))\n",
    "\n",
    "    # Convert grayscale image to RGB since the model expects three channels\n",
    "    face_image = cv2.cvtColor(face_image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    face_image = img_to_array(face_image)\n",
    "    face_image = np.expand_dims(face_image, axis=0)\n",
    "    face_image /= 255.0  # Normalize pixel values to be between 0 and 1\n",
    "\n",
    "    start_predict_time = time.time()  # Time prediction start\n",
    "    # Make emotion prediction\n",
    "    predictions = emotion_model.predict(face_image)\n",
    "    end_predict_time = time.time()  # Time prediction end\n",
    "\n",
    "    # Calculate the prediction time\n",
    "    prediction_time = end_predict_time - start_predict_time\n",
    "\n",
    "    emotion_name = class_names[np.argmax(predictions)]  # Get the emotion name\n",
    "    confidence = np.max(predictions) * 100  # Get the confidence score\n",
    "\n",
    "    # Print prediction and time\n",
    "    print(f\"{emotion_name} ({confidence:.2f}%)\")\n",
    "    print(f\"1/1 [==============================] - {prediction_time:.3f}s {int(1/prediction_time)}ms/step\")\n",
    "\n",
    "    return emotion_name, confidence\n",
    "\n",
    "# Open a connection to the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    start_time = time.time()  # Start time for measuring frame rate\n",
    "\n",
    "    # Read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # If the frame is not retrieved successfully, exit loop\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    detections = detector.detect(frame)\n",
    "\n",
    "    # Check if any faces were detected\n",
    "    if len(detections) > 0:\n",
    "        for detection in detections:\n",
    "            x1, y1, x2, y2, _ = detection\n",
    "            face_image = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "\n",
    "            # Classify emotion for the extracted face\n",
    "            emotion_name, confidence = classify_emotion(face_image)\n",
    "\n",
    "            # Draw a bounding box, emotion name, and confidence on the frame\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "            text = f\"{emotion_name} ({confidence:.2f}%)\"\n",
    "            cv2.putText(frame, text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with FPS\n",
    "    end_time = time.time()\n",
    "    fps = 1 / (end_time - start_time)\n",
    "    cv2.putText(frame, f\"FPS: {fps:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "     # Display the frame\n",
    "    cv2.namedWindow(\"Emotion Detection\", cv2.WND_PROP_FULLSCREEN)\n",
    "    cv2.setWindowProperty(\"Emotion Detection\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "    \n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f029a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0165375a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "fear (95.76%)\n",
      "1/1 [==============================] - 1.713s 0ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "disgust (97.38%)\n",
      "1/1 [==============================] - 0.105s 9ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "disgust (97.31%)\n",
      "1/1 [==============================] - 0.097s 10ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "fear (65.64%)\n",
      "1/1 [==============================] - 0.080s 12ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "fear (92.48%)\n",
      "1/1 [==============================] - 0.077s 12ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "fear (95.97%)\n",
      "1/1 [==============================] - 0.080s 12ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "fear (96.20%)\n",
      "1/1 [==============================] - 0.087s 11ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "fear (97.23%)\n",
      "1/1 [==============================] - 0.139s 7ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "disgust (73.01%)\n",
      "1/1 [==============================] - 0.146s 6ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "disgust (92.65%)\n",
      "1/1 [==============================] - 0.151s 6ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "disgust (98.67%)\n",
      "1/1 [==============================] - 0.166s 6ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "surprise (86.18%)\n",
      "1/1 [==============================] - 0.154s 6ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "surprise (99.95%)\n",
      "1/1 [==============================] - 0.160s 6ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "happy (100.00%)\n",
      "1/1 [==============================] - 0.145s 6ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "happy (63.06%)\n",
      "1/1 [==============================] - 0.153s 6ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "fear (63.32%)\n",
      "1/1 [==============================] - 0.154s 6ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "disgust (46.10%)\n",
      "1/1 [==============================] - 0.136s 7ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "disgust (97.22%)\n",
      "1/1 [==============================] - 0.147s 6ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "disgust (67.68%)\n",
      "1/1 [==============================] - 0.144s 6ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "disgust (57.29%)\n",
      "1/1 [==============================] - 0.144s 6ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "disgust (95.82%)\n",
      "1/1 [==============================] - 0.146s 6ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "disgust (55.85%)\n",
      "1/1 [==============================] - 0.135s 7ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "sad (75.29%)\n",
      "1/1 [==============================] - 0.149s 6ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "disgust (69.51%)\n",
      "1/1 [==============================] - 0.146s 6ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "happy (99.89%)\n",
      "1/1 [==============================] - 0.137s 7ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "disgust (37.63%)\n",
      "1/1 [==============================] - 0.131s 7ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "disgust (64.67%)\n",
      "1/1 [==============================] - 0.148s 6ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "neutral (95.24%)\n",
      "1/1 [==============================] - 0.151s 6ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "disgust (66.32%)\n",
      "1/1 [==============================] - 0.145s 6ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "sad (70.93%)\n",
      "1/1 [==============================] - 0.149s 6ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "sad (84.22%)\n",
      "1/1 [==============================] - 0.136s 7ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "surprise (69.32%)\n",
      "1/1 [==============================] - 0.129s 7ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "surprise (58.05%)\n",
      "1/1 [==============================] - 0.152s 6ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "angry (86.00%)\n",
      "1/1 [==============================] - 0.141s 7ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "fear (44.41%)\n",
      "1/1 [==============================] - 0.151s 6ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "fear (98.07%)\n",
      "1/1 [==============================] - 0.136s 7ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "fear (56.79%)\n",
      "1/1 [==============================] - 0.141s 7ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "happy (71.68%)\n",
      "1/1 [==============================] - 0.139s 7ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "fear (75.76%)\n",
      "1/1 [==============================] - 0.137s 7ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "happy (53.73%)\n",
      "1/1 [==============================] - 0.150s 6ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "disgust (99.81%)\n",
      "1/1 [==============================] - 0.149s 6ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "disgust (45.06%)\n",
      "1/1 [==============================] - 0.163s 6ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "fear (78.69%)\n",
      "1/1 [==============================] - 0.142s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import face_detection\n",
    "import time\n",
    "\n",
    "# Load the pre-trained emotion classification model\n",
    "emotion_model = load_model('D:\\\\Human Emotion Recognition\\\\RAF DB\\\\RAF Models\\\\best_model_efficientnet_raf_after_aug.h5')\n",
    "\n",
    "# Initialize the face detection model\n",
    "detector = face_detection.build_detector(\"RetinaNetResNet50\", confidence_threshold=.5, nms_iou_threshold=.3)\n",
    "\n",
    "# Specify the class names explicitly as strings in the order they were trained\n",
    "class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "def classify_emotion(face_image):\n",
    "    # Convert color image to grayscale for the emotion recognition model\n",
    "    face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize the face image to the new target size of 100 x 100\n",
    "    face_image = cv2.resize(face_image, (100, 100))\n",
    "\n",
    "    # Convert grayscale image to RGB since the model expects three channels\n",
    "    face_image = cv2.cvtColor(face_image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    face_image = img_to_array(face_image)\n",
    "    face_image = np.expand_dims(face_image, axis=0)\n",
    "    face_image /= 255.0  # Normalize pixel values to be between 0 and 1\n",
    "\n",
    "    start_predict_time = time.time()  # Time prediction start\n",
    "    # Make emotion prediction\n",
    "    predictions = emotion_model.predict(face_image)\n",
    "    end_predict_time = time.time()  # Time prediction end\n",
    "\n",
    "    # Calculate the prediction time\n",
    "    prediction_time = end_predict_time - start_predict_time\n",
    "\n",
    "    emotion_name = class_names[np.argmax(predictions)]  # Get the emotion name\n",
    "    confidence = np.max(predictions) * 100  # Get the confidence score\n",
    "\n",
    "    # Print prediction and time\n",
    "    print(f\"{emotion_name} ({confidence:.2f}%)\")\n",
    "    print(f\"1/1 [==============================] - {prediction_time:.3f}s {int(1/prediction_time)}ms/step\")\n",
    "\n",
    "    return emotion_name, confidence\n",
    "\n",
    "# Open a connection to the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    start_time = time.time()  # Start time for measuring frame rate\n",
    "\n",
    "    # Read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # If the frame is not retrieved successfully, exit loop\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    detections = detector.detect(frame)\n",
    "\n",
    "    # Check if any faces were detected\n",
    "    if len(detections) > 0:\n",
    "        for detection in detections:\n",
    "            x1, y1, x2, y2, _ = detection\n",
    "            face_image = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "\n",
    "            # Classify emotion for the extracted face\n",
    "            emotion_name, confidence = classify_emotion(face_image)\n",
    "\n",
    "            # Draw a bounding box, emotion name, and confidence on the frame\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "            text = f\"{emotion_name} ({confidence:.2f}%)\"\n",
    "            cv2.putText(frame, text, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with FPS\n",
    "    end_time = time.time()\n",
    "    fps = 1 / (end_time - start_time)\n",
    "    cv2.putText(frame, f\"FPS: {fps:.2f}\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "     # Display the frame\n",
    "    cv2.namedWindow(\"Emotion Detection\", cv2.WND_PROP_FULLSCREEN)\n",
    "    cv2.setWindowProperty(\"Emotion Detection\", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "    \n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff86ee",
   "metadata": {},
   "source": [
    "# Emotion Prediction from Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89c0ae8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.jpg, .jpeg, .png', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import face_detection\n",
    "from ipywidgets import FileUpload, Output\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "# Load the pre-trained emotion classification model\n",
    "emotion_model = load_model('D:\\\\Human Emotion Recognition\\Emotion Recognition Datasets\\\\Affectnet Models\\\\patch_model_affectnet.h5')  # Replace with the actual path\n",
    "\n",
    "# Initialize the face detection model\n",
    "detector = face_detection.build_detector(\"RetinaNetResNet50\", confidence_threshold=.5, nms_iou_threshold=.3)\n",
    "\n",
    "# Specify the class names explicitly as strings in the order they were trained\n",
    "emotion_labels = ['neutral', 'happy', 'sad', 'surprise', 'fear', 'disgust', 'angry']\n",
    "\n",
    "def classify_emotion(face_image):\n",
    "    # Check if the image is empty\n",
    "    if face_image is None or face_image.size == 0:\n",
    "        return \"No face detected\", 0  # Return a message indicating no face detected\n",
    "    \n",
    "    # Check if the image is not in grayscale\n",
    "    if len(face_image.shape) == 3 and face_image.shape[2] != 1:\n",
    "        # Convert color image to grayscale\n",
    "        face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert grayscale image to RGB\n",
    "    face_image = cv2.cvtColor(face_image, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Resize the face image to (224, 224)\n",
    "    face_image = cv2.resize(face_image, (224, 224))\n",
    "    face_image = img_to_array(face_image)\n",
    "    face_image = np.expand_dims(face_image, axis=0)\n",
    "    face_image /= 255.0  # Normalize pixel values\n",
    "\n",
    "    # Make emotion prediction\n",
    "    predictions = emotion_model.predict(face_image)\n",
    "    emotion_name = emotion_labels[np.argmax(predictions)]  # Get the emotion name\n",
    "    confidence = np.max(predictions) * 100  # Confidence as a percentage\n",
    "\n",
    "    return emotion_name, confidence  # Return the emotion name and confidence\n",
    "\n",
    "# Function to process and display the uploaded image\n",
    "def process_uploaded_image(change):\n",
    "    uploader = change['owner']\n",
    "    uploaded_image = uploader.value\n",
    "    \n",
    "    # Read the uploaded image\n",
    "    image_data = uploaded_image[list(uploaded_image.keys())[0]]\n",
    "    image = Image.open(io.BytesIO(image_data['content']))\n",
    "    \n",
    "    # Convert the PIL image to a NumPy array\n",
    "    input_image = np.array(image)\n",
    "    \n",
    "    # Detect faces in the input image\n",
    "    detections = detector.detect(input_image)\n",
    "    \n",
    "    # Process detected faces\n",
    "    for detection in detections:\n",
    "        x1, y1, x2, y2, _ = detection  # Note the removal of [0] here\n",
    "        face_image = input_image[int(y1):int(y2), int(x1):int(x2)]\n",
    "\n",
    "        # Classify emotion for the extracted face\n",
    "        emotion_name, confidence = classify_emotion(face_image)\n",
    "        \n",
    "        # Draw a bounding box on the input image\n",
    "        cv2.rectangle(input_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "        # Display emotion name with reduced font size on the input image\n",
    "        text = \"{} ({:.2f}%)\".format(emotion_name, confidence)\n",
    "        font_scale = 0.5  # Adjust the font scale (reduce it to make the text smaller)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX  # Font style\n",
    "        font_thickness = 1  # Thickness of the text\n",
    "        font_color = (0, 255, 0)  # Font color (green)\n",
    "\n",
    "        # Calculate the text size to position it properly\n",
    "        text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "        text_x = int(x1 + (x2 - x1) / 2 - text_size[0] / 2)\n",
    "        text_y = int(y1 - 10)\n",
    "\n",
    "        # Draw the bounding box on the input image\n",
    "        cv2.rectangle(input_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "        # Display the emotion name and confidence on the image\n",
    "        cv2.putText(input_image, text, (text_x, text_y), font, font_scale, font_color, font_thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "        # Display the emotion name and confidence on the screen\n",
    "        print(text)\n",
    "\n",
    "    # Display the input image with bounding boxes and emotion labels using Matplotlib\n",
    "    plt.figure(figsize=(20, 20))  # Set the figure size to display the image larger\n",
    "    plt.imshow(input_image)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Save the displayed image to a file\n",
    "    plt.savefig('D:\\\\Human Emotion Recognition\\\\Output predicted images\\\\displayed_image.png', bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create a FileUpload widget for image uploading\n",
    "uploader = FileUpload(accept='.jpg, .jpeg, .png', multiple=False)\n",
    "\n",
    "# Register the callback function to process the uploaded image\n",
    "uploader.observe(process_uploaded_image, 'value')\n",
    "\n",
    "# Create an Output widget for displaying the uploaded image\n",
    "output = Output()\n",
    "\n",
    "# Display the widgets\n",
    "display(uploader, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04115b3",
   "metadata": {},
   "source": [
    "# Emotion Prediction in Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16e9c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5745eb9e314e81a05382343ba506c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='video/*', description='Upload video')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c04aed3ff304412be7cd87f7a557a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import face_detection\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "\n",
    "# Load the pre-trained emotion classification model\n",
    "emotion_model = load_model('D:\\\\Human Emotion Recognition\\\\Emotion Recognition Datasets\\\\Affectnet Models\\\\patch_model_raf.h5')\n",
    "\n",
    "# Initialize the face detection model\n",
    "detector = face_detection.build_detector(\"RetinaNetResNet50\", confidence_threshold=.5, nms_iou_threshold=.3)\n",
    "\n",
    "class_names = ['neutral', 'happy', 'sad', 'surprise', 'fear', 'disgust', 'angry']\n",
    "\n",
    "def classify_emotion(face_image):\n",
    "    # Check if the image is empty\n",
    "    if face_image is None or face_image.size == 0:\n",
    "        return \"No face detected\", 0  # Return a message indicating no face detected\n",
    "    face_image = cv2.resize(face_image, (100, 100))\n",
    "    face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n",
    "    face_image = cv2.cvtColor(face_image, cv2.COLOR_GRAY2RGB)\n",
    "    face_image = img_to_array(face_image)\n",
    "    face_image = np.expand_dims(face_image, axis=0)\n",
    "    face_image /= 255.0\n",
    "    predictions = emotion_model.predict(face_image)\n",
    "    emotion_index = np.argmax(predictions)\n",
    "    return class_names[emotion_index], predictions[0][emotion_index] * 100\n",
    "\n",
    "upload_video = widgets.FileUpload(description=\"Upload video\", accept='video/*', multiple=False)\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "def process_frame(frame):\n",
    "    detections = detector.detect(frame)\n",
    "    for detection in detections:\n",
    "        x1, y1, x2, y2, _ = detection\n",
    "        face_image = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "        emotion_name, confidence = classify_emotion(face_image)\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"{emotion_name} {confidence:.2f}%\", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    return frame\n",
    "\n",
    "def process_video(file_data):\n",
    "    with output_widget:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            content = file_data['content']\n",
    "            stream = io.BytesIO(content)\n",
    "            temp_video_path = \"temp_video.mp4\"\n",
    "            with open(temp_video_path, \"wb\") as temp_file:\n",
    "                temp_file.write(content)\n",
    "\n",
    "            cap = cv2.VideoCapture(temp_video_path)\n",
    "            frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "            frame_count = 0\n",
    "\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                frame_count += 1\n",
    "\n",
    "                if not ret:\n",
    "                    break  # End of video\n",
    "\n",
    "                processed_frame = process_frame(frame)\n",
    "                cv2.putText(processed_frame, f\"FPS: {frame_rate:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                display(Image.fromarray(frame_rgb))\n",
    "\n",
    "                time.sleep(0.033)\n",
    "                clear_output(wait=True)\n",
    "\n",
    "            cap.release()\n",
    "            print(\"Finished processing video.\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error processing the video.\")\n",
    "\n",
    "def on_video_upload_change(change):\n",
    "    if upload_video.value:\n",
    "        process_video(next(iter(upload_video.value.values())))\n",
    "\n",
    "# Observe changes in the upload widgets\n",
    "upload_video.observe(on_video_upload_change, names='value')\n",
    "\n",
    "# Display widgets for video upload and output\n",
    "display(upload_video, output_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda227d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
